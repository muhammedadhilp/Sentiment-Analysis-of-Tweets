# -*- coding: utf-8 -*-
"""Sentiment Analysis on Twitter Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1h-mDvBLuHbkboWGIUEJPccBhsDtR5q
"""

import pandas as pd
import numpy as np
import csv
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(
    '/content/training.1600000.processed.noemoticon.csv',
    encoding='latin',  # Use 'latin1' encoding or 'utf-8' as appropriate
    names=['sentiment', 'id', 'date', 'query', 'user', 'tweet'])
df.head()

df = df.sample(frac=1)
df = df[:20000]
df.shape

df.dtypes

# removing unneccesary cols
df = df.drop(['id','date','query','user'],axis = 1)
df.head(10)

# sentiment ranges from 0 , 4. 0 being negative and 4 being positive so we need to change it into 0 = neg and 1 = pos
df['sentiment']=df['sentiment'].replace(4,1)
print(df['sentiment'].unique())

# Check for any null values
df.isnull().sum()
# no null values

# converting pandas object to str
df['tweet'] = df['tweet'].astype('str')

# checking no of positive and negative tagged tweets
pos = df['sentiment'][df.sentiment==1]
neg = df['sentiment'][df.sentiment==0]
print(f"Total number of tweets:{df.shape[0]}")
print(f"Total number of positive tweets:{len(pos)}")
print(f"Total number of negative tweets:{len(neg)}")

sns.countplot(x='sentiment',data=df)
# almost equal distirbution

# Import necessary NLTK modules
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Explanation: Stop words are common words like "the", "a", "an", "in"
# that are typically ignored in natural language processing (NLP) tasks
# to focus on more meaningful content words.
nltk.download('stopwords')

# Define the stop words set for the English language
stopword = set(stopwords.words('english'))

# Print the list of stopwords
print(stopword)

import warnings
warnings.filterwarnings('ignore')
import re
import string
import pickle
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Define URL and username patterns for removal
url_pattern = r"((http://)[^ ]*|(https://)[^ ]*|(www\.)[^ ]*)"
user_pattern = r'@[^\s]+'

# Words to remove
remove_words = 'amp,today,tomorrow,going,girl'

# Stopwords set for filtering
stopword = set(stopwords.words('english'))

# Function to process and clean a single tweet
def process_tweets(tweet):
    # Normalize contractions and common phrases
    contractions = {
        r"he's": "he is", r"there's": "there is", r"We're": "We are", r"That's": "That is",
        r"won't": "will not", r"they're": "they are", r"Can't": "cannot", r"wasn't": "was not",
        r"aren't": "are not", r"isn't": "is not", r"What's": "What is", r"haven't": "have not",
        r"hasn't": "has not", r"He's": "He is", r"It's": "It is", r"You're": "You are",
        r"I'M": "I am", r"shouldn't": "should not", r"wouldn't": "would not", r"I've": "I have",
        r"can't": "cannot", r"don't": "do not", r"doesn't": "does not", r"i'm": "I am",
        r"I'll": "I will", r"I'd": "I would", r"Let's": "let us", r"you're": "you are",
        r"ain't": "am not", r"you've": "you have", r"couldn't": "could not", r"they've": "they have"
    }
    for contraction, full_form in contractions.items():
        tweet = re.sub(contraction, full_form, tweet)

    # Replace some abbreviations and slang
    abbreviations = {
        r"some1": "someone", r"yrs": "years", r"hrs": "hours", r"2morow|2moro": "tomorrow",
        r"2day": "today", r"4got|4gotten": "forget", r"b-day|bday": "birthday",
        r"hahah|hahaha|hahahaha": "haha", r"lmao|lolz|rofl": "lol", r"thanx|thnx": "thanks"
    }
    for abbr, full_form in abbreviations.items():
        tweet = re.sub(abbr, full_form, tweet)

    # Convert to lowercase
    tweet = tweet.lower()

    # Remove URLs, usernames, and some specific words
    tweet = re.sub(url_pattern, '', tweet)
    tweet = re.sub(user_pattern, '', tweet)
    tweet = re.sub(remove_words, '', tweet)

    # Remove punctuation
    tweet = tweet.translate(str.maketrans('', '', string.punctuation))

    # Tokenize words
    tokens = word_tokenize(tweet)

    # Remove stopwords and lemmatize words
    word_lemmatizer = WordNetLemmatizer()
    final_words = [
        word_lemmatizer.lemmatize(w) for w in tokens if w not in stopword and len(w) > 1
    ]

    # Return the cleaned tweet
    return ' '.join(final_words)

# Dictionary of abbreviations
abbreviations = {
    "$": "dollar",
    "â‚¬": "euro",
    "4ao": "for adults only",
    "a.m": "before midday",
    "a3": "anytime anywhere anyplace",
    "aamof": "as a matter of fact",
    "acct": "account",
    "adih": "another day in hell",
    "afaic": "as far as I am concerned",
    "afaict": "as far as I can tell",
    "afaik": "as far as I know",
    "afair": "as far as I remember",
    "afk": "away from keyboard",
    "app": "application",
    "approx": "approximately",
    "apps": "applications",
    "asap": "as soon as possible",
    "asl": "age, sex, location",
    "atk": "at the keyboard",
    "ave.": "avenue",
    "aymm": "are you my mother",
    "ayor": "at your own risk",
    "b&b": "bed and breakfast",
    "b+b": "bed and breakfast",
    "b.c": "before Christ",
    "b2b": "business to business",
    "b2c": "business to customer",
    "b4": "before",
    "b4n": "bye for now",
    "b@u": "back at you",
    "bae": "before anyone else",
    "bak": "back at keyboard",
    "bbbg": "bye bye be good",
    "bbc": "British Broadcasting Corporation",
    "bbias": "be back in a second",
    "bbl": "be back later",
    "bbs": "be back soon",
    "be4": "before",
    "bfn": "bye for now",
    "blvd": "boulevard",
    "bout": "about",
    "brb": "be right back",
    "bros": "brothers",
    "brt": "be right there",
    "bsaaw": "big smile and a wink",
    "btw": "by the way",
    "bwl": "bursting with laughter",
    "c/o": "care of",
    "cet": "central european time",
    "cf": "compare",
    "cia": "central intelligence agency",
    "csl": "can not stop laughing",
    "cu": "see you",
    "cul8r": "see you later",
    "cv": "curriculum vitae",
    "cwot": "complete waste of time",
    "cya": "see you",
    "cyt": "see you tomorrow",
    "dae": "does anyone else",
    "dbmib": "do not bother me I am busy",
    "diy": "do it yourself",
    "dm": "direct message",
    "dwh": "during work hours",
    "e123": "easy as one two three",
    "eet": "eastern european time",
    "eg": "example",
    "embm": "early morning business meeting",
    "encl": "enclosed",
    "encl.": "enclosed",
    "etc": "and so on",
    "faq": "frequently asked questions",
    "fawc": "for anyone who cares",
    "fb": "facebook",
    "fc": "fingers crossed",
    "fig": "figure",
    "fimh": "forever in my heart",
    "ft.": "feet",
    "ft": "featuring",
    "ftl": "for the loss",
    "ftw": "for the win",
    "fwiw": "for what it's worth",
    "fyi": "for your information",
    "g9": "genius",
    "gahoy": "get a hold of yourself",
    "gal": "get a life",
    "gcse": "general certificate of secondary education",
    "gfn": "gone for now",
    "gg": "good game",
    "gl": "good luck",
    "glhf": "good luck have fun",
    "gmt": "greenwich mean time",
    "gmta": "great minds think alike",
    "gn": "good night",
    "g.o.a.t": "greatest of all time",
    "goat": "greatest of all time",
    "goi": "get over it",
    "gps": "global positioning system",
    "gr8": "great",
    "gratz": "congratulations",
    "gyal": "girl",
    "h&c": "hot and cold",
    "hp": "horsepower",
    "hr": "hour",
    "hrh": "his royal highness",
    "ht": "height",
    "ibrb": "I will be right back",
    "ic": "I see",
    "icq": "I seek you",
    "icymi": "in case you missed it",
    "idc": "I do not care",
    "idgadf": "I do not give a damn fuck",
    "idgaf": "I do not give a fuck",
    "idk": "I do not know",
    "ie": "that is",
    "i.e": "that is",
    "ifyp": "I feel your pain",
    "IG": "Instagram",
    "iirc": "if I remember correctly",
    "ilu": "I love you",
    "ily": "I love you",
    "imho": "in my humble opinion",
    "imo": "in my opinion",
    "imu": "I miss you",
    "iow": "in other words",
    "irl": "in real life",
    "j4f": "just for fun",
    "jic": "just in case",
    "jk": "just kidding",
    "jsyk": "just so you know",
    "l8r": "later",
    "lb": "pound",
    "lbs": "pounds",
    "ldr": "long distance relationship",
    "lmao": "laugh my ass off",
    "lmfao": "laugh my fucking ass off",
    "lol": "laughing out loud",
    "ltd": "limited",
    "ltns": "long time no see",
    "m8": "mate",
    "mf": "motherfucker",
    "mfs": "motherfuckers",
    "mfw": "my face when",
    "mofo": "motherfucker",
    "mph": "miles per hour",
    "mr": "mister",
    "mrw": "my reaction when",
    "ms": "miss",
    "mte": "my thoughts exactly",
    "nagi": "not a good idea",
    "nbc": "national broadcasting company",
    "nbd": "no big deal",
    "nfs": "not for sale",
    "ngl": "not going to lie",
    "nhs": "national health service",
    "nrn": "no reply necessary",
    "nsfl": "not safe for life",
    "nsfw": "not safe for work",
    "nth": "nice to have",
    "nvr": "never",
    "nyc": "New York City",
    "oc": "original content",
    "og": "original",
    "ohp": "overhead projector",
    "oic": "oh I see",
    "omdb": "over my dead body",
    "omg": "oh my god",
    "omw": "on my way",
    "p.a": "per annum",
    "p.m": "after midday",
    "pm": "prime minister",
    "poc": "people of color",
    "pov": "point of view",
    "pp": "pages",
    "ppl": "people",
    "prw": "parents are watching",
    "ps": "postscript",
    "pt": "point",
    "ptb": "please text back",
    "pto": "please turn over",
    "qpsa": "what happens",
    "ratchet": "rude",
    "rbtl": "read between the lines",
    "rlrt": "real life retweet",
    "rofl": "rolling on the floor laughing",
    "roflol": "rolling on the floor laughing out loud",
    "rotflmao": "rolling on the floor laughing my ass off",
    "rt": "retweet",
    "ruok": "are you ok",
    "sfw": "safe for work",
    "sk8": "skate",
    "smh": "shake my head",
    "sq": "square",
    "srsly": "seriously",
    "ssdd": "same stuff different day",
    "tbh": "to be honest",
    "tbs": "tablespoonful",
    "tbsp": "tablespoonful",
    "tfw": "that feeling when",
    "thks": "thank you",
    "tho": "though",
    "thx": "thank you",
    "tia": "thanks in advance",
    "til": "today I learned",
    "tl;dr": "too long I did not read",
    "tldr": "too long I did not read",
    "tmb": "tweet me back",
    "tnt": "til next time",
    "ttyl": "talk to you later",
    "u": "you",
    "u2": "you too",
    "uok": "you ok",
    "vip": "very important person",
    "vs": "versus",
    "w/": "with",
    "w/o": "without",
    "w8": "wait",
    "wassup": "what is up",
    "wb": "welcome back",
    "wtf": "what the fuck",
    "wyd": "what are you doing",
    "xoxo": "hugs and kisses",
    "yolo": "you only live once",
    "yup": "yes"
}

def abbreviate(tweet):
    words = tweet.split()
    t = [abbreviations[w.lower()] if w.lower() in abbreviations else w for w in words]
    return ' '.join(t)

df['processed_tweet'] = df['tweet'].apply(lambda x:process_tweets(x))
df['processed_tweet'] = df['processed_tweet'].apply(lambda x:abbreviate(x))
print("Text processing completed")
df

# Removing words with less than 3 letters
df['processed_tweet'] = df['processed_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w) > 3]))
df

from sklearn.utils import shuffle
df =  shuffle(df).reset_index(drop=True)

# Tokenization
tokenized_tweets = df['processed_tweet'].apply(lambda x: x.split())
tokenized_tweets.head(10)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)
text_counts = cv.fit_transform(df['processed_tweet'].values.astype('U'))

from sklearn.model_selection import train_test_split
X=text_counts
y=df['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=19)

from sklearn.naive_bayes import ComplementNB
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from math import *
cnb = ComplementNB()
cnb.fit(X_train, y_train)
cross_cnb = cross_val_score(cnb, X, y,n_jobs = -1)
print("Cross Validation score = ",cross_cnb)
print ("Train accuracy ={:.2f}%".format(cnb.score(X_train,y_train)*100))
print ("Test accuracy ={:.2f}%".format(cnb.score(X_test,y_test)*100))
train_acc_cnb=cnb.score(X_train,y_train)
test_acc_cnb=cnb.score(X_test,y_test)

#plotting the best parameters
import matplotlib.patches as mpatches
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import matplotlib.pyplot as plt
data_cnb = [train_acc_cnb,test_acc_cnb]
labels = ['Train Accuracy','Test Accuracy']
plt.xticks(range(len(data_cnb)), labels)
plt.ylabel('Accuracy')
plt.title('Accuracy plot with best parameters')
plt.bar(range(len(data_cnb)), data_cnb,color=['blue','darkorange'])
Train_acc = mpatches.Patch(color='blue', label='Train_acc')
Test_acc = mpatches.Patch(color='darkorange', label='Test_acc')
plt.legend(handles=[Train_acc, Test_acc],loc='best')
plt.gcf().set_size_inches(8, 8)
plt.show()

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Predict the test dataset
y_pred_cnb = cnb.predict(X_test)

# Print the confusion matrix
cm = confusion_matrix(y_test, y_pred_cnb)
print("Confusion Matrix:")
print(cm)

# Print additional evaluation metrics
print("\nClassification Report:")
print(classification_report(y_test, y_pred_cnb))

print("Accuracy Score:")
print(accuracy_score(y_test, y_pred_cnb))
# Accuracy Score:
# 0.71575

from sklearn.metrics import *

print("F1 score ={:.2f}%".format(f1_score(y_test, y_pred_cnb, average="macro")*100))
f1_cnb=f1_score(y_test, y_pred_cnb, average="macro")
print("Precision score ={:.2f}%".format(precision_score(y_test, y_pred_cnb, average="macro")*100))
precision_cnb=precision_score(y_test, y_pred_cnb, average="macro")
print("Recall score ={:.2f}%".format(recall_score(y_test, y_pred_cnb, average="macro")*100))
recall_cnb=recall_score(y_test, y_pred_cnb, average="macro")

# F1 score =71.57%
# Precision score =71.64%
# Recall score =71.62%